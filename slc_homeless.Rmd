---
title: 'STAT 651: Project'
author: "Skyler Gray"
date: "November 9, 2021"
output: word_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE)

library(ggmap)
library(tidyverse)
library(lubridate)
library(flextable)
library(kableExtra)
library(cowplot)
library(rstan)

# Set random seed
set.seed(1122)

## Settings for Stan
nCores <- parallel::detectCores() - 1
options(mc.cores = nCores)          # Use all available cores.
rstan_options(auto_write = TRUE)    # Cache compiled code.
rstan_options(javascript = FALSE)

# Set ggplot theme
mytheme <- theme_bw()
theme_set(mytheme)

# Picture saving settings
pic_width <- 7
pic_height <- 4
pic_unit <- 'in'
```

```{r read-data}
# COVID-19 county new cases data
covid <- read_csv('data/covid19_cases_saltlakecounty.csv')

# Unfiltered homeless data; for plotting only
homeless_init <- read_csv('data/slc_homeless_only_locations.csv') %>%
  mutate(district = as.character(district))
# Filtered homeless data; for analysis
homeless <- read_csv('data/homeless_requests.csv',
                     col_types = cols(district = col_character())) %>%
  select(-date_closed) %>%
  rename(date = date_created)

# 2010 Census tract data 
pop_2019 <- read_csv('data/slc_population_2019_est_tracts.csv') %>%
  select(-map_code)
inc_2019 <- read_csv('data/slc_income_2019_est_tracts.csv') %>%
  select(-map_code)

# Combine all data together
combined <- homeless %>%
  left_join(pop_2019, by = 'tract') %>%
  left_join(inc_2019, by = 'tract') %>%
  left_join(covid, by = 'date') %>%
  # Filter out the 8 observations in the airport tract; no income there
  filter(tract != 980000) %>%
  mutate(logincome = log(income)) %>%
  select(id, date, long, lat, cases_avg7, district, 
         tract, density, logincome, days_open)
```


# Introduction

```{r request-map}
# Add request locations on SLC map
bb <- make_bbox(lon = long, lat = lat, data = homeless)
mymap <- get_map(location = bb, zoom = 11,
                 maptype = "hybrid", source = "google",
                 color = 'bw')

# Unfiltered homelessness report map
p <- ggmap(mymap) + 
  geom_point(data = homeless_init, 
             aes(x = long, y = lat, col = district),
             alpha = 0.6,
             size = 2) +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank()) +
  scale_color_brewer(type = 'qual', palette = 2) +
  scale_x_continuous(limits = c(min(homeless_init$long)-0.001,
                                max(homeless_init$long)+0.001)) +
  scale_y_continuous(limits = c(min(homeless_init$lat)-0.001,
                                max(homeless_init$lat)+0.001)) +
  labs(col = 'District')

ggsave('plots/reports_map.png',
       plot = p,
       device = 'png',
       dpi = 300,
       width = pic_width, 
       height = pic_height,
       units = pic_unit)

# Filtered (data used for the analysis) homelessness report map
p <- ggmap(mymap) + 
  geom_point(data = homeless, 
             aes(x = long, y = lat, col = district),
             alpha = 0.6,
             size = 2) +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank()) +
  scale_color_brewer(type = 'qual', palette = 2) +
  scale_x_continuous(limits = c(min(homeless$long)-0.001,
                                max(homeless$long)+0.001)) +
  scale_y_continuous(limits = c(min(homeless$lat)-0.001,
                                max(homeless$lat)+0.001)) +
  labs(col = 'District')

ggsave('plots/reports_map_filtered.png',
       plot = p,
       device = 'png',
       dpi = 300,
       width = pic_width, 
       height = pic_height,
       units = pic_unit)
```


# The Data

```{r}
combined %>%
  select(district, cases_avg7, density, logincome, days_open) %>%
  mutate(across(cases_avg7:logincome, function(x) round(x, 1))) %>%
  head(4) %>%
  flextable() %>%
  set_header_labels(district = 'District',
                    cases_avg7 = 'Average New COVID Cases',
                    density = 'Population Density',
                    logincome = 'Log(Median Household Income)',
                    days_open = 'Days Open') %>%
  autofit() %>%
  theme_zebra()
```


# Exploratory Data Analysis

```{r inside-analysis, eval=FALSE}
# Sample size is not large enough to predict differences in neighborhoods
homeless %>%
  group_by(district, neighborhood) %>%
  summarize(n = n(),
            avg_date = mean(date),
            min_date = min(date)) %>%
  arrange(neighborhood) %>%
  View()
```

```{r sample-size-mean-days}
combined %>%
  group_by(district) %>%
  summarize(n = n(),
            days_open = mean(days_open) %>%
              round(2)) %>%
  flextable() %>%
  set_header_labels(district = 'District',
                    days_open = 'Days Open') %>%
  autofit() %>%
  theme_zebra()
```

```{r eda-plots}
p <- combined %>%
  ggplot(aes(days_open)) +
  geom_histogram() +
  labs(x = 'Days Open', y = 'Density')

p1 <- combined %>%
  ggplot(aes(date, log(days_open))) +
  geom_jitter(alpha = 0.5) +
  labs(x = 'Date', y = 'log(Days Open)')

p2 <- combined %>%
  ggplot(aes(district, log(days_open))) +
  geom_boxplot() +
  labs(x = 'District', y = 'log(Days Open)')

p3 <- combined %>%
  ggplot(aes(cases_avg7, log(days_open))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = 'lm',
              se = FALSE) +
  labs(x = 'New COVID Cases (Rolling 7-Day Avg)',
       y = 'log(Days Open)')

p4 <- combined %>%
  ggplot(aes(logincome, log(days_open))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = 'lm',
              se = FALSE) +
  labs(x = 'log(Income)', y = 'log(Days Open)')

p5 <- combined %>%
  ggplot(aes(density, log(days_open))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = 'lm',
              se = FALSE) +
  labs(x = 'Population Density (Pop per Sq. Mile)',
       y = 'log(Days Open)')

# Save plots
plot_grid(p1, p3, p4, p5, 
          nrow = 2, byrow = TRUE) %>%
  ggsave(filename = 'plots/eda_cont_predictors.png',
         plot = .,
         device = 'png',
         dpi = 300,
         width = pic_width, 
         height = pic_height,
         units = pic_unit)

ggsave('plots/eda_days_histogram.png',
       plot = p,
       device = 'png',
       dpi = 300,
       width = pic_width, 
       height = pic_height,
       units = pic_unit)

ggsave('plots/eda_days_districts.png',
       plot = p2,
       device = 'png',
       dpi = 300,
       width = pic_width, 
       height = pic_height,
       units = pic_unit)
```


# Fit Model: GLM

```{r scale-data}
X <- model.matrix(~ district + density + logincome + cases_avg7,
                  data = combined)
contvars_idx <- 8:10
vars_scaled <- scale(X[,contvars_idx])
X[,contvars_idx] <- vars_scaled
y <- combined$days_open
```

```{r trad-glm-fit}
fit <- glm(days_open ~ X - 1,
           data = combined,
           family = poisson)
```


# Fit Model: Slice Sampler

```{r}
loglike <- function(beta) {
  sum( dpois(y, exp(X%*%beta), log = TRUE) )
}
logprior <- function(beta) {
  mu_0 <- 0
  tau2_0 <- 1/100
  
  sum( dnorm(beta, mu_0, sqrt(1/tau2_0), log = TRUE) )
}
logpost <- function(beta) {
  loglike(beta) + logprior(beta)
}

# Stabilize computations of log(posterior) because even the max val is super small
betahat <- optim(coef(fit), function(beta) -logpost(beta))
logpost_max <- -betahat$value

logpost_adj <- function(beta) {
  # subtract maximum logpost value to stabilize computations
  loglike(beta) + logprior(beta) - logpost_max
}
```

```{r sample-slice}
doit <- function(ndraws = 10000, width = rep(1, 10), warmup = 1000) {
  n_evals <- 0
  f <- function(pars) {
    n_evals <<- n_evals + 1
    logpost_adj(pars)
  }
  draws <- matrix(0, nrow = ndraws, ncol = 10)
  current <- coef(fit) # Start with GLM fit
  f_current <- f(current)
  
  for (i in 1:ndraws) {
    for (j in 1:ncol(draws)) {
      # Slice sample beta1
      y_slice <- log(runif(1, 0, exp(f_current)))
      l <- current[j] - runif(1)*width[j]
      l_prop <- current
      l_prop[j] <- l
      u <- l + width[j]
      u_prop <- current
      u_prop[j] <- u
      
      while (y_slice < f(l_prop)) {
        l <- l - width[j]
        l_prop[j] <- l
      } 
      while (y_slice < f(u_prop)) {
        u <- u + width[j]
        u_prop[j] <- u
      }
      while (TRUE) {
        candidate <- runif(1, l, u)
        cand_prop <- current; cand_prop[j] <- candidate
        f_candidate <- f(cand_prop)
        if (y_slice > f_candidate ) {
          if ( candidate < current[j] ) l <- candidate
          else u <- candidate
        } else break
      }
      current[j] <- candidate
      f_current <- f_candidate
    }
    draws[i,] <- current
  }
  
  return(list(draws = draws[-c(1:warmup),],
              evals_per_draw = n_evals/ndraws))
}

ndraws <- 1e4
warmup <- 1000
width <- rep(0.08, 10)
mychains <- 4
test <- replicate(mychains, 
                  doit(ndraws = ndraws, width = width, warmup = warmup)$draws,
                  simplify = FALSE)

# Thin
thin_idx <- seq(4, mychains*(ndraws-warmup), by = 4)
draws <- do.call(rbind, test) %>%
  .[thin_idx,]

ess <- floor( coda::effectiveSize(draws) )

rhat <- numeric(10)
mydraws <- matrix(0, nrow = (ndraws-warmup)/mychains, ncol = mychains)
for (i in 1:10) {
  for (j in 0:3) {
    mydraws[,j] <- draws[( j*(ndraws-warmup)/mychains + 1 ):( (j+1)*(ndraws-warmup)/mychains ), i]
  }
  rhat[i] <- Rhat(mydraws)
}
```

```{r}
coef_names <- colnames(X)
tbl_slice <- data.frame(coef = coef_names,
                   est = colMeans(draws),
                   apply(draws, 2, 
                         function(x) quantile(x, c(0.025, 0.975))) %>%
                     t(),
                   ESS = ess,
                   rhat = rhat)

# Rescale continuous betas
col_idx <- 2:4
sds <- attr(vars_scaled, 'scaled:scale')
tbl_slice[8, col_idx] <- tbl_slice[8, col_idx] / sds[1] * 1000
tbl_slice[9, col_idx] <- tbl_slice[9, col_idx] / sds[2]
tbl_slice[10, col_idx] <- tbl_slice[10, col_idx] / sds[3]
tbl_slice[,2:6] <- round(tbl_slice[,2:6], 4)

tbl_slice %>%
  flextable() %>%
  autofit() %>%
  theme_zebra()
```



# Fit Model: Stan

```{r stan-model}
N <- nrow(combined)

# Prior values
mu_0 <- 0
tau2_0 <- 1/100

# Fit Stan model
m <- stan_model(model_code = readLines("model.stan"))
data <- list(N = N,
             y = y, 
             X = X,
             mu_0 = mu_0,
             tau2_0 = tau2_0)
fit_stan <- sampling(m, data = data,
                     iter = 10000, warmup = 1000,
                     chains = nCores)
```


```{r stan-model-diagnostics}
# Extract beta samples
beta <- extract(fit_stan)$beta

print(fit_stan, probs = c(0.05, 0.95))

Index <- 1:nrow(beta)
p1 <- ggplot(mapping = aes(Index, beta[,1])) +
  geom_line()
p2 <- ggplot(mapping = aes(Index, beta[,2])) +
  geom_line()
p3 <- ggplot(mapping = aes(Index, beta[,8])) +
  geom_line()
p4 <- ggplot(mapping = aes(Index, beta[,10])) +
  geom_line()

# Save plots
plot_grid(p1, p2, p3, p4, 
          nrow = 2, byrow = TRUE) %>%
  ggsave(filename = 'plots/results_trace.png',
         plot = .,
         device = 'png',
         dpi = 300,
         width = pic_width, 
         height = pic_height,
         units = pic_unit)
```

```{r analysis-districts}
# Plot district effects
beta_districts <- beta[,1:7]
for (j in 2:7) {
  beta_districts[,j] <- beta_districts[,1] + beta_districts[,j]
}
est <- colMeans(beta_districts)
ci <- apply(beta_districts, 2, function(x) quantile(x, c(0.025, 0.975)))
lwr <- ci[1,]
upr <- ci[2,]

beta_districts <- data.frame(district = as.character(1:7),
                             est = exp(est),
                             lwr = exp(lwr),
                             upr = exp(upr))
p <- beta_districts %>%
  ggplot(aes(district, est, ymin = lwr, ymax = upr)) +
  geom_point() +
  geom_errorbar() +
  labs(x = 'District',
       y = 'Mean Days Request is Open',
       title = 'Posterior mean and credible interval estimates')

ggsave('plots/post_district_ci.png',
       plot = p,
       device = 'png',
       dpi = 300,
       width = pic_width, 
       height = pic_height,
       units = pic_unit)
```

```{r}
coef_names <- colnames(X)
tbl_stan <- data.frame(coef = coef_names,
                   est = colMeans(beta),
                   apply(beta, 2, 
                         function(x) quantile(x, c(0.025, 0.975))) %>%
                     t(),
                   ESS = coda::effectiveSize(beta) %>%
                     round(),
                   rhat = Rhat(beta))

# Rescale continuous betas
col_idx <- 2:4
sds <- attr(vars_scaled, 'scaled:scale')
tbl_stan[8, col_idx] <- tbl_stan[8, col_idx] / sds[1] * 1000
tbl_stan[9, col_idx] <- tbl_stan[9, col_idx] / sds[2]
tbl_stan[10, col_idx] <- tbl_stan[10, col_idx] / sds[3]
tbl_stan[,2:6] <- round(tbl_stan[,2:6], 4)

tbl_stan %>%
  flextable() %>%
  autofit() %>%
  theme_zebra()
```



# Extras

## SLC Shapefile Data

```{r shapefile, eval=FALSE}
library(rgdal)
library(rgeos) # For tidy() function
library(broom) #contains tidy() function which converts polygons to data.frame

path <- 'data/SLC_Census_Tracts_2010'
myShp <- readOGR(dsn = path, layer = 'CensusTracts2010')

myShp@data$id <- rownames(myShp@data) #Assign ID to each polygon
myShp.df <- tidy(myShp, region = "id") #Convert polygon info to data.frame()
myShp.df <- merge(myShp.df, myShp@data, by = "id") #Merge data w/polygon data.frame

myShp.df %>%
  ggplot(aes(x=long, y=lat, group = group)) + 
  geom_polygon(color="black", fill='gray')
```

## MH Algorithm

```{r metropolis, eval=FALSE}
# Rookie implementation of Metropolis algorithm
#  NOT efficient; slice sampler works much better

ndraws <- 10000
nbetas <- 10
accept <- 0
draws <- matrix(0, nrow = ndraws, ncol = nbetas)
state <- rep(0.5, nbetas)
for (i in 1:ndraws) {
  proposal <- state + rnorm(nbetas, 0, 0.1)
  metropolis <- logpost(proposal) - logpost(state)
  if (log(runif(1)) < metropolis) {
    draws[i,] <- proposal
    state <- proposal
    accept <- accept + 1
  } else draws[i,] <- state
}
plot(draws[,4], type = 'l')
accept
coda::effectiveSize(draws)
```
